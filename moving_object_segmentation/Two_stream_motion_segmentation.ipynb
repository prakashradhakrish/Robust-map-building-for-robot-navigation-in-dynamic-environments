{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# for dataloader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "import glob\n",
    "from torchvision import transforms,datasets\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "#saving model\n",
    "from datetime import datetime\n",
    "\n",
    "#Testing\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder which takes 3 channel input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,num_classes = 2):\n",
    "        super().__init__()\n",
    "        num_classes = 2\n",
    "        model1 = models.vgg16(pretrained=True)\n",
    "        features1, classifier1 = list(model1.features.children()), list(model1.classifier.children())\n",
    "\n",
    "        self.features1_3 = nn.Sequential(*features1[: 17])\n",
    "        self.features1_4 = nn.Sequential(*features1[17: 24])\n",
    "        self.features1_5 = nn.Sequential(*features1[24:])\n",
    "        \n",
    "        \n",
    "    def forward(self,img):\n",
    "        pool1_3 = self.features1_3(img)\n",
    "        pool1_4 = self.features1_4(pool1_3)\n",
    "        pool1_5 = self.features1_5(pool1_4)\n",
    "        return pool1_3,pool1_4,pool1_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder which takes 6 channel input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_2(nn.Module):\n",
    "    def __init__(self,num_classes = 2):\n",
    "        super().__init__()\n",
    "        num_classes = 2\n",
    "        model1 = models.vgg16(pretrained=True)\n",
    "        features1, classifier1 = list(model1.features.children()), list(model1.classifier.children())\n",
    "        \n",
    "        self.feature1_2 = nn.Sequential(nn.Conv2d(6,64,kernel_size=3, stride=1, padding=1),nn.ReLU(inplace=True))\n",
    "        self.features1_3 = nn.Sequential(*features1[2: 17])\n",
    "        self.features1_4 = nn.Sequential(*features1[17: 24])\n",
    "        self.features1_5 = nn.Sequential(*features1[24:])\n",
    "        \n",
    "        \n",
    "    def forward(self,img):\n",
    "        pool1_2 = self.feature1_2(img)\n",
    "        pool1_3 = self.features1_3(pool1_2)\n",
    "        pool1_4 = self.features1_4(pool1_3)\n",
    "        pool1_5 = self.features1_5(pool1_4)\n",
    "        return pool1_3,pool1_4,pool1_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self,n_classes = 2):\n",
    "        super().__init__()    \n",
    "        self.score_pool3 = nn.Conv2d(256,n_classes, kernel_size=1)\n",
    "        self.score_pool4 = nn.Conv2d(512,n_classes, kernel_size=1)\n",
    "\n",
    "        self.upsampling2 = nn.ConvTranspose2d(n_classes, n_classes, kernel_size=4,stride=2, bias=False)\n",
    "        self.upsampling8 = nn.ConvTranspose2d(n_classes, n_classes, kernel_size=16,stride=8, bias=False)\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Conv2d(512, n_classes, kernel_size=1), nn.Sigmoid())\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def forward(self,pool3,pool4,pool5,x_size):\n",
    "        o = self.classifier(pool5)\n",
    "        o = self.upsampling2(o)\n",
    "\n",
    "        o2 = self.score_pool4(pool4)\n",
    "        o = o[:, :, 1:1 + o2.size()[2], 1:1 + o2.size()[3]]\n",
    "        o = o + o2\n",
    "\n",
    "        o = self.upsampling2(o)\n",
    "\n",
    "        o2 = self.score_pool3(pool3)\n",
    "        o = o[:, :, 1:1 + o2.size()[2], 1:1 + o2.size()[3]]\n",
    "        o = o + o2\n",
    "\n",
    "        o = self.upsampling8(o)\n",
    "        cx = int((o.shape[3] - x_size[3]) / 2)\n",
    "        cy = int((o.shape[2] - x_size[2]) / 2)\n",
    "        o = o[:, :, cy:cy + x_size[2], cx:cx + x_size[3]]\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModNet model with 1 x 1 convolution with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1_1(in_features, out_features):\n",
    "    return nn.Sequential(nn.Conv2d(in_channels=in_features, out_channels=out_features, kernel_size=1),nn.Dropout(p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class modnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modnet,self).__init__()\n",
    "        num_classes = 2\n",
    "        self.encoder1 = encoder()\n",
    "        self.encoder2 = encoder()\n",
    "        \n",
    "        self.decoder1 = decoder()\n",
    "        self.decoder2 = decoder()\n",
    "        self.conv1_3 = conv_1_1(256,256)\n",
    "        self.conv1_4 = conv_1_1(512,512)\n",
    "        self.conv1_5 = conv_1_1(512,512)\n",
    "\n",
    "    def forward(self, rgb, of):\n",
    "        x_size = rgb.size()\n",
    "        #combined_flow = torch.cat((spatial_stem, motion_stem),dim=1)\n",
    "        # encoder 1\n",
    "        pool1_3, pool1_4, pool1_5 = self.encoder1(rgb)\n",
    "        # encoder 2\n",
    "        pool2_3, pool2_4, pool2_5 = self.encoder2(of)\n",
    "        # combined features\n",
    "        pool3 = self.conv1_3(pool1_3 + pool2_3)\n",
    "        pool4 = self.conv1_4(pool1_4 + pool2_4)\n",
    "        pool5 = self.conv1_5(pool1_5 + pool2_5)\n",
    "        #pool3 = pool1_3 + pool2_3\n",
    "        #pool4 = pool1_4 + pool2_4\n",
    "        #pool5 = pool1_5 + pool2_5\n",
    "        # decoder 1\n",
    "        spatial_out = self.decoder1(pool3,pool4,pool5,x_size)\n",
    "        #decoder 2\n",
    "        motion_out = self.decoder2(pool3,pool4,pool5,x_size)\n",
    "        \n",
    "        return spatial_out,motion_out\n",
    "    \n",
    "    \n",
    "model = modnet()\n",
    "# testing the output of the model\n",
    "out = model(torch.rand(1,3,448,448),torch.rand(1,3,448,448))\n",
    "print(out[0].shape,out[1].shape)\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    model=model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModNet model - Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modnet,self).__init__()\n",
    "        num_classes = 2\n",
    "        self.encoder1 = encoder()\n",
    "        self.encoder2 = encoder()\n",
    "        \n",
    "        self.decoder1 = decoder()\n",
    "        self.decoder2 = decoder()\n",
    "\n",
    "\n",
    "    def forward(self, rgb, of):\n",
    "        x_size = rgb.size()\n",
    "        #combined_flow = torch.cat((spatial_stem, motion_stem),dim=1)\n",
    "        # encoder 1\n",
    "        pool1_3, pool1_4, pool1_5 = self.encoder1(rgb)\n",
    "        # encoder 2\n",
    "        pool2_3, pool2_4, pool2_5 = self.encoder2(of)\n",
    "        # combined features\n",
    "        pool3 = pool1_3 + pool2_3\n",
    "        pool4 = pool1_4 + pool2_4\n",
    "        pool5 = pool1_5 + pool2_5\n",
    "        # decoder 1\n",
    "        spatial_out = self.decoder1(pool3,pool4,pool5,x_size)\n",
    "        #decoder 2\n",
    "        motion_out = self.decoder2(pool3,pool4,pool5,x_size)\n",
    "        \n",
    "        return spatial_out,motion_out\n",
    "    \n",
    "    \n",
    "model = modnet()\n",
    "# testing the output of the model\n",
    "out = model(torch.rand(1,3,448,448),torch.rand(1,3,448,448))\n",
    "print(out[0].shape,out[1].shape)\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    model=model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModNet model - Singledecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modnet_single_decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modnet_single_decoder,self).__init__()\n",
    "        num_classes = 2\n",
    "        self.encoder1 = encoder()\n",
    "        self.encoder2 = encoder()\n",
    "        \n",
    "        self.decoder1 = decoder(n_classes = 3)\n",
    "\n",
    "\n",
    "    def forward(self, rgb, of):\n",
    "        x_size = rgb.size()\n",
    "        #combined_flow = torch.cat((spatial_stem, motion_stem),dim=1)\n",
    "        # encoder 1\n",
    "        pool1_3, pool1_4, pool1_5 = self.encoder1(rgb)\n",
    "        # encoder 2\n",
    "        pool2_3, pool2_4, pool2_5 = self.encoder2(of)\n",
    "        # combined features\n",
    "        pool3 = pool1_3 + pool2_3\n",
    "        pool4 = pool1_4 + pool2_4\n",
    "        pool5 = pool1_5 + pool2_5\n",
    "        # decoder 1\n",
    "        out = self.decoder1(pool3,pool4,pool5,x_size)\n",
    "        #decoder 2\n",
    "        #motion_out = self.decoder2(pool3,pool4,pool5,x_size)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "model = modnet_single_decoder()\n",
    "# testing the output of the model\n",
    "out = model(torch.rand(1,3,370,1226),torch.rand(1,3,370,1226))\n",
    "print(out.shape)\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    model=model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ModNet model - Configuration 2 - 6ch combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modnet_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modnet_2,self).__init__()\n",
    "        num_classes = 2\n",
    "        self.encoder1 = encoder()\n",
    "        self.encoder2 = encoder_2()\n",
    "        \n",
    "        self.decoder1 = decoder()\n",
    "        self.decoder2 = decoder()\n",
    "\n",
    "    def forward(self, rgb, of):\n",
    "        x_size = rgb.size()\n",
    "        combined_flow = torch.cat((rgb, of),dim=1)\n",
    "        #print(combined_flow.shape)\n",
    "        # encoder 1\n",
    "        pool1_3, pool1_4, pool1_5 = self.encoder1(rgb)\n",
    "        # encoder 2\n",
    "        pool2_3, pool2_4, pool2_5 = self.encoder2(combined_flow)\n",
    "        # combined features\n",
    "        pool3 = pool1_3 + pool2_3\n",
    "        pool4 = pool1_4 + pool2_4\n",
    "        pool5 = pool1_5 + pool2_5\n",
    "        # decoder 1\n",
    "        spatial_out = self.decoder1(pool3,pool4,pool5,x_size)\n",
    "        #decoder 2\n",
    "        motion_out = self.decoder2(pool3,pool4,pool5,x_size)\n",
    "        \n",
    "        return spatial_out,motion_out\n",
    "    \n",
    "    \n",
    "model = modnet_2()\n",
    "# testing the output of the model\n",
    "out = model(torch.rand(1,3,224,224),torch.rand(1,3,224,224))\n",
    "print(out[0].shape,out[1].shape)\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    model=model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class synDataset(VisionDataset):\n",
    "    def __init__(self, inp_dim):   \n",
    "        self.transforms = transforms.ToTensor()\n",
    "        self.inp_dim = inp_dim\n",
    "        self.rgb_dir = glob.glob(\"/homebackup/dataset/2011_09_30/Kitti/dataset_odometry_annotation/dataset_for_training/images/*.png\")\n",
    "        self.rgb_dir.sort()\n",
    "        self.flow_dir = glob.glob(\"/homebackup/dataset/2011_09_30/Kitti/dataset_odometry_annotation/dataset_for_training/flownet/*.png\")\n",
    "        self.flow_dir.sort()\n",
    "        self.mask1_dir = glob.glob(\"/homebackup/dataset/2011_09_30/Kitti/dataset_odometry_annotation/dataset_for_training/mask1/*.png\")\n",
    "        self.mask1_dir.sort()\n",
    "        self.mask2_dir = glob.glob(\"/homebackup/dataset/2011_09_30/Kitti/dataset_odometry_annotation/dataset_for_training/mask2/*.png\")\n",
    "        self.mask2_dir.sort()\n",
    "                  \n",
    "    def __getitem__(self, index):\n",
    "        rgb = Image.open(self.rgb_dir[index]).convert('RGB')\n",
    "        rgb = rgb.resize(self.inp_dim)\n",
    "        flow = Image.open(self.flow_dir[index]).convert('RGB')\n",
    "        flow = flow.resize(self.inp_dim)\n",
    "        mask1 = Image.open(self.mask1_dir[index])\n",
    "        mask1 = mask1.resize(self.inp_dim)\n",
    "        mask2 = Image.open(self.mask2_dir[index])\n",
    "        mask2 = mask2.resize(self.inp_dim)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            rgb = self.transforms(rgb)\n",
    "            flow = self.transforms(flow)\n",
    "            mask1 = self.transforms(mask1)\n",
    "            mask2 = self.transforms(mask2)\n",
    "        return rgb, flow, mask1, mask2\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len(self.rgb_dir)\n",
    "    \n",
    "\n",
    "batch_size = 1\n",
    "dataloaders = {\n",
    "    'train': DataLoader(synDataset(inp_dim=(224,224)), batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "### Dice loss + Binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dice_loss(pred, target):\n",
    "    pred = pred.contiguous().view(pred.size()[0], -1)\n",
    "    target = target.contiguous().view(target.size()[0], -1).float()\n",
    "\n",
    "    a = torch.sum(pred * target, 1)\n",
    "    b = torch.sum(pred * pred, 1) + 0.001\n",
    "    c = torch.sum(target * target, 1) + 0.001\n",
    "    d = (2 * a) / (b + c)\n",
    "    return 1 - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "#from loss import dice_loss\n",
    "\n",
    "def calc_loss(pred, target, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target).type(dtype)\n",
    "\n",
    "    pred = torch.sigmoid(pred).type(dtype)\n",
    "    dice = dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def label_conv(labels,targetimage, mask1=True):\n",
    "    if mask1:\n",
    "        # background        \n",
    "        labels[:,0,:,:] = (targetimage > 0)==0\n",
    "        labels[:,0,:,:] = (targetimage == 0)==1  \n",
    "        # mask1\n",
    "        labels[:,1,:,:] = (targetimage > 0)==1\n",
    "        labels[:,1,:,:] = (targetimage == 0)==0\n",
    "    else:\n",
    "        # background        \n",
    "        labels[:,0,:,:] = (targetimage < 1)==1\n",
    "        labels[:,0,:,:] = (targetimage == 1)==0  \n",
    "        # mask1\n",
    "        labels[:,1,:,:] = (targetimage < 1)==0\n",
    "        labels[:,1,:,:] = (targetimage == 1)==1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label conv for single decoder combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_conv(labels,targetimage, mask1=True):\n",
    "    if mask1:\n",
    "        # background        \n",
    "        labels[:,0,:,:] = (targetimage != 0)==0\n",
    "        labels[:,0,:,:] = (targetimage == 0)==1  \n",
    "        # mask1\n",
    "        labels[:,1,:,:] = (targetimage == 1)==1\n",
    "        labels[:,1,:,:] = (targetimage != 1)==0\n",
    "        # mask1\n",
    "        labels[:,2,:,:] = (targetimage == 2)==2\n",
    "        labels[:,2,:,:] = (targetimage != 2)==0\n",
    "    else:\n",
    "        # background        \n",
    "        labels[:,0,:,:] = (targetimage < 1)==1\n",
    "        labels[:,0,:,:] = (targetimage == 1)==0  \n",
    "        # mask1\n",
    "        labels[:,1,:,:] = (targetimage < 1)==0\n",
    "        labels[:,1,:,:] = (targetimage == 1)==1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weight calculation for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = np.array([0.0,0.0,0.0])\n",
    "\n",
    "for data in tqdm(dataloaders['train']):\n",
    "    imgs, flowimage, mask1, mask2 = data\n",
    "    mask1 = mask1.detach().squeeze(0).permute(1,2,0).numpy().astype('int8')\n",
    "    mask1[mask1>0.5]=1\n",
    "    mask1[mask1<0.5]=0\n",
    "    mask2 = mask2.detach().squeeze(0).permute(1,2,0).numpy().astype('int8')\n",
    "    mask2[mask2>0.5]=1\n",
    "    mask2[mask2<0.5]=0\n",
    "    true_masks = mask1+mask2\n",
    "    (unique, counts) = np.unique(true_masks, return_counts=True)\n",
    "    frequencies = np.asarray((unique, counts))\n",
    "    for i in range(frequencies.shape[1]):\n",
    "        class_weight[frequencies[0,i]] += frequencies[1,i]\n",
    "class_weight = class_weight.min()/class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_values =[]\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"LR\", param_group['lr'])\n",
    "    scheduler.step()\n",
    "    for i, data in enumerate(dataloaders['train']):\n",
    "        inputimage, flowimage, mask1, mask2 = data\n",
    "        \n",
    "        if gpu_available:\n",
    "            inputimage = inputimage.cuda()\n",
    "            flowimage = flowimage.cuda()\n",
    "            mask1 = mask1.cuda()\n",
    "            mask2 = mask2.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputimage,flowimage)\n",
    "        \n",
    "        labels = torch.empty_like(outputs[0],dtype=torch.float)\n",
    "        lab_channel1 = (label_conv(labels,mask1,True))\n",
    "        labels = torch.empty_like(outputs[1],dtype=torch.float)\n",
    "        lab_channel2 = (label_conv(labels,mask2,True))\n",
    "\n",
    "        if gpu_available:\n",
    "            lab_channel1 = lab_channel1.cuda()\n",
    "            lab_channel2 = lab_channel2.cuda()\n",
    "\n",
    "\n",
    "        #loss_dec1 = dice_loss(torch.sigmoid(outputs[0]),lab_channel1)\n",
    "        #loss_dec2 = dice_loss(torch.sigmoid(outputs[1]),lab_channel2)\n",
    "        loss_dec1 = calc_loss(outputs[0],lab_channel1)\n",
    "        loss_dec2 = calc_loss(outputs[1],lab_channel2)\n",
    "        losses = loss_dec1+loss_dec2\n",
    "\n",
    "        #loss += lmbd * reg_loss\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += losses.item()\n",
    "        loss_values.append(losses.item())\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.10f' %(epoch + 1, i + 1, running_loss / 10))\n",
    "            #loss_values.append(running_loss / co)\n",
    "            running_loss = 0.0\n",
    "    epoch_loss = running_loss / len(dataloaders['train'])\n",
    "    print('epoch loss: %.4f'%(epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training single decoder configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values =[]\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"LR\", param_group['lr'])\n",
    "    scheduler.step()\n",
    "    for i, data in enumerate(dataloaders['train']):\n",
    "        inputimage, flowimage, mask1, mask2 = data\n",
    "        mask1[mask1>0.5]=1\n",
    "        mask2[mask2>0.5]=1\n",
    "        mask = mask1+mask2\n",
    "        \n",
    "        if gpu_available:\n",
    "            inputimage = inputimage.cuda()\n",
    "            flowimage = flowimage.cuda()\n",
    "            mask = mask.cuda()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputimage,flowimage)\n",
    "        \n",
    "        labels = torch.empty_like(outputs,dtype=torch.float)\n",
    "        lab_channel1 = (label_conv(labels,mask,True))\n",
    "\n",
    "        if gpu_available:\n",
    "            lab_channel1 = lab_channel1.cuda()\n",
    "\n",
    "        #loss_dec1 = dice_loss(torch.sigmoid(outputs[0]),lab_channel1)\n",
    "        #loss_dec2 = dice_loss(torch.sigmoid(outputs[1]),lab_channel2)\n",
    "        losses = []\n",
    "        labels = [Variable(label.cuda()) for label in lab_channel1.squeeze(0)]\n",
    "        #print(len(labels))\n",
    "        for pair in zip(outputs, labels):          \n",
    "            #print(pair[0].shape,pair[1].shape)\n",
    "            #losses.append(criterion(pair[0].unsqueeze(0), pair[1].unsqueeze(0).long()))\n",
    "            losses.append(F.cross_entropy(pair[0].unsqueeze(0), \n",
    "                                          pair[1].unsqueeze(0).long()))\n",
    "        #losses = criterion(outputs,lab_channel1.long().squeeze(0))\n",
    "        \n",
    "\n",
    "        loss = 0\n",
    "        for w, l in zip(class_weight, losses):\n",
    "            loss += w*l\n",
    "\n",
    "        #loss += lmbd * reg_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        loss_values.append(loss.item())\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.10f' %(epoch + 1, i + 1, running_loss / 10))\n",
    "            #loss_values.append(running_loss / co)\n",
    "            running_loss = 0.0\n",
    "    epoch_loss = running_loss / len(dataloaders['train'])\n",
    "    print('epoch loss: %.4f'%(epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'cpkt_modnet_'+ datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")+'_15ep_singledec_bceweight_reddata.pth'\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checkpoint_modnet_2021_06_27_004245_newdataset - not clear\n",
    "# checkpoint_modnet_2021_06_29_133752_0_5_bc3_448res - professor\n",
    "# checkpoint_modnet_2021_06_30_022156_newar_bothloss_448 - works fine with false positive\n",
    "# cpkt_modnet_2021_07_04_070214_15ep_cfg2_combloss_reddata.pth - better shape in most of the cases\n",
    "# cpkt_modnet_2021_07_07_083336_30ep_cfg1_combloss_reddata_orgdim - 120mb - poor performance\n",
    "# cpkt_modnet_2021_07_09_072004_25ep_cfg1_bce_reddata_orgdim - not clear shape\n",
    "# cpkt_modnet_2021_07_09_001123_30ep_cfg1_closs_dloss_reddata_orgdim - poor performance\n",
    "# cpkt_modnet_2021_07_10_075834_30ep_cfg2_closs_reddata_orgdim.pth - Better\n",
    "\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load('./checkpoint_modnet_2021_06_11_013934.pth'))\n",
    "#model.load_state_dict(torch.load('./checkpoint_modnet_2021_06_21_221234_newflownet_olddataset.pth'))\n",
    "#model.load_state_dict(torch.load('./cpkt_modnet_2021_07_06_071731_30ep_cfg1_combloss_reddata_orgdim.pth')) # ok\n",
    "#model.load_state_dict(torch.load('./cpkt_modnet_2021_07_09_072004_25ep_cfg1_bce_reddata_orgdim.pth'))\n",
    "#model.load_state_dict(torch.load('./cpkt_modnet_2021_07_09_144506_30ep_singledec_bceweight_reddata_orgdim.pth'))\n",
    "\n",
    "model.load_state_dict(torch.load('./cpkt_modnet_2021_07_18_184304_30ep_cfg1_closs_actdata_orgdim.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class synDataset_test(VisionDataset):\n",
    "    def __init__(self, inp_dim):   \n",
    "        self.transforms = transforms.ToTensor()\n",
    "        self.inp_dim = inp_dim\n",
    "        self.rgb_dir = glob.glob(\"/homebackup/dataset/2011_09_30/Kitti/Thesis_evaluation/07/image_3/*.png\")\n",
    "        self.rgb_dir.sort()\n",
    "        self.flow_dir = glob.glob(\"/homebackup/dataset/2011_09_30/Kitti/Thesis_evaluation/07/flownet/*.png\")\n",
    "        self.flow_dir.sort()\n",
    "        #self.mask_dir = glob.glob(\"/homebackup/dataset/2011_09_30/downloads/images-20210610T200228Z-001/test/mask/*.png\")\n",
    "        #self.mask_dir.sort()\n",
    "\n",
    "                  \n",
    "    def __getitem__(self, index):\n",
    "        rgb = Image.open(self.rgb_dir[index]).convert('RGB')\n",
    "        rgb = rgb.resize(self.inp_dim)\n",
    "        flow = Image.open(self.flow_dir[index]).convert('RGB')\n",
    "        flow = flow.resize(self.inp_dim)\n",
    "        #mask = Image.open(self.mask_dir[index])\n",
    "        #mask = mask.resize(self.inp_dim)\n",
    "\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            rgb = self.transforms(rgb)\n",
    "            flow = self.transforms(flow)\n",
    "            #mask = self.transforms(mask)\n",
    "\n",
    "        return rgb, flow\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len(self.rgb_dir)\n",
    "    \n",
    "\n",
    "batch_size = 1\n",
    "dataloaders = {\n",
    "    'test': DataLoader(synDataset_test(inp_dim=(1226,370)), batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = model.eval()\n",
    "for i, data in enumerate(dataloaders['test']):\n",
    "    inputimage, flowimage= data\n",
    "\n",
    "    if gpu_available:\n",
    "        inputimage = inputimage.cuda()\n",
    "        flowimage = flowimage.cuda()\n",
    "        #mask = mask.cuda()\n",
    "\n",
    "    outputs = model(inputimage,flowimage)\n",
    "    break\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "pred1 = torch.sigmoid(outputs[0])\n",
    "pred1[pred1<0.3]=0\n",
    "pred1 = torch.argmax(pred1, dim=1)\n",
    "pred2 = torch.sigmoid(outputs[1])\n",
    "pred2[pred2<0.3]=0\n",
    "pred2 = torch.argmax(pred2, dim=1)\n",
    "#printing image\n",
    "fig,((ax1,ax2),(ax4,ax5))=plt.subplots(2,2,figsize=(20,20),facecolor='w')\n",
    "ax1.imshow(pred1.cpu().detach().numpy().squeeze(0),  interpolation='none',cmap='jet')\n",
    "ax2.imshow(pred2.cpu().detach().numpy().squeeze(0),  interpolation='none',cmap='jet')\n",
    "#ax3.imshow(mask.cpu().detach().numpy().squeeze(0).squeeze(0),  interpolation='none',cmap='jet')\n",
    "ax4.imshow(inputimage.squeeze(0).cpu().permute(1,2,0))\n",
    "ax5.imshow(flowimage.squeeze(0).cpu().permute(1,2,0))\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "#ax3.axis('off')\n",
    "ax4.axis('off')\n",
    "ax5.axis('off')\n",
    "ax1.set_title('prediction_spatial',fontsize=20)\n",
    "ax2.set_title('prediction_motion',fontsize=20)\n",
    "\n",
    "#ax3.set_title('GT_motion',fontsize=20)\n",
    "ax4.set_title('Input',fontsize=20)\n",
    "ax5.set_title('Flow',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = model.eval()\n",
    "for i, data in enumerate(dataloaders['test']):\n",
    "    inputimage, flowimage= data\n",
    "\n",
    "    if gpu_available:\n",
    "        inputimage = inputimage.cuda()\n",
    "        flowimage = flowimage.cuda()\n",
    "        #mask = mask.cuda()\n",
    "\n",
    "    outputs = model(inputimage,flowimage)\n",
    "    break\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#pred2 = torch.sigmoid(outputs)\n",
    "#pred2[pred2<0.1]=0\n",
    "pred2 = torch.argmax(outputs, dim=1)\n",
    "#printing image\n",
    "fig,((ax2,ax4,ax5))=plt.subplots(3,1,figsize=(20,20),facecolor='w')\n",
    "#ax1.imshow(pred1.cpu().detach().numpy().squeeze(0),  interpolation='none',cmap='jet')\n",
    "ax2.imshow(pred2.cpu().detach().numpy().squeeze(0),  interpolation='none',cmap='jet')\n",
    "#ax3.imshow(mask.cpu().detach().numpy().squeeze(0).squeeze(0),  interpolation='none',cmap='jet')\n",
    "ax4.imshow(inputimage.squeeze(0).cpu().permute(1,2,0))\n",
    "ax5.imshow(flowimage.squeeze(0).cpu().permute(1,2,0))\n",
    "\n",
    "ax2.axis('off')\n",
    "ax4.axis('off')\n",
    "ax5.axis('off')\n",
    "ax2.set_title('prediction_motion',fontsize=20)\n",
    "ax4.set_title('Input',fontsize=20)\n",
    "ax5.set_title('Flow',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.sigmoid(outputs).squeeze(0)[2,:,:].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result without loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputimage = Image.open('/homebackup/dataset/2011_09_30/Kitti/Thesis_evaluation/07/image_3/000056.png').convert('RGB')\n",
    "orig_size = inputimage.size\n",
    "inputimage = inputimage.resize((1226,370))\n",
    "inputimage = transform(inputimage)\n",
    "\n",
    "flowimage = Image.open('/homebackup/dataset/2011_09_30/Kitti/Thesis_evaluation/07/flownet/000056-vis.png').convert('RGB')\n",
    "flowimage = flowimage.resize((1226,370))\n",
    "flowimage = transform(flowimage)\n",
    "\n",
    "if gpu_available:\n",
    "    inputimage = inputimage.cuda()\n",
    "    flowimage = flowimage.cuda()\n",
    "\n",
    "outputs = model(inputimage.unsqueeze(0),flowimage.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowimage1 = cv2.imread('/homebackup/dataset/2011_09_30/Kitti/Thesis_evaluation/07/flownet/000978-vis.png')\n",
    "flowimage2= cv2.imread('/homebackup/dataset/2011_09_30/Kitti/Thesis_evaluation/07/flownet/000979-vis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(flowimage1-flowimage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = torch.argmax(outputs[1], dim=1)\n",
    "flow = np.ones((flowimage.detach().cpu().permute(1,2,0).numpy()).shape)\n",
    "print(np.max(flowimage.detach().cpu().permute(1,2,0).numpy()),np.min(flowimage.detach().cpu().permute(1,2,0).numpy()))\n",
    "flow[(flowimage.detach().cpu().permute(1,2,0).numpy())>0.97]=0\n",
    "flow = np.mean((flow),axis=2)\n",
    "flow[flow>0]=1\n",
    "prediction = pred2.detach().cpu().numpy().squeeze(0)*flow\n",
    "\n",
    "prediction = np.array(prediction, dtype='uint8')\n",
    "prediction = cv2.resize(prediction, (orig_size), interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = torch.sigmoid(outputs[0])\n",
    "pred1[pred1<0.6]=0\n",
    "pred1 = torch.argmax(pred1, dim=1)\n",
    "pred2 = torch.sigmoid(outputs[1])\n",
    "pred2[pred2<0.6]=0\n",
    "pred2 = torch.argmax(pred2, dim=1)\n",
    "flow = np.ones((flowimage.detach().cpu().permute(1,2,0).numpy()).shape)\n",
    "print(np.max(flowimage.detach().cpu().permute(1,2,0).numpy()),np.min(flowimage.detach().cpu().permute(1,2,0).numpy()))\n",
    "flow[(flowimage.detach().cpu().permute(1,2,0).numpy())>0.9]=0\n",
    "flow = np.mean((flow),axis=2)\n",
    "flow[flow>0]=1\n",
    "prediction = pred2.detach().cpu().numpy().squeeze(0)*flow\n",
    "\n",
    "prediction = np.array(prediction, dtype='uint8')\n",
    "prediction = cv2.resize(prediction, (orig_size), interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(inputimage.detach().cpu().permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(flowimage.detach().cpu().permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(pred1.detach().cpu().numpy().squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(pred2.detach().cpu().numpy().squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solo_mask = cv2.imread('/homebackup/dataset/2011_09_30/Kitti/Thesis_evaluation/07/track/000066_track.png',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOU calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_calc(target,prediction,i):\n",
    "    temp_target = np.zeros(target.shape)\n",
    "    temp_target[target==i]=1\n",
    "    intersection = np.logical_and(temp_target, prediction)\n",
    "    union = np.logical_or(temp_target, prediction)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining solov2 ouput with motion segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solo_mask = cv2.imread(solo_track_path,0)\n",
    "mask = np.ones(solo_mask.shape)\n",
    "for i in list(np.unique(solo_mask)):\n",
    "    if i !=0:\n",
    "        if i in list(np.unique(prediction*solo_mask)):\n",
    "            iou = iou_calc(solo_mask,prediction,i)\n",
    "            if iou>0.001:\n",
    "                mask[solo_mask==i]=155\n",
    "            else:\n",
    "                mask[solo_mask==i]=55\n",
    "        else:\n",
    "            mask[solo_mask==i]=55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask = Image.fromarray(mask.astype('uint8'))\n",
    "final_mask = final_mask.resize((1226,370))\n",
    "final_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
